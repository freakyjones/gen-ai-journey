# 2025-09-18  Transformers

Timestamp: 2025-09-18T18:33:21+05:30

- Key idea: sequence-to-sequence modeling via self-attention.
- Why it matters: parallelizable training, long-range dependencies.
- Note: encoder/decoder stacks; positional encodings; multi-head attention.

